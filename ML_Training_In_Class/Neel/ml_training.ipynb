{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02349c2d",
   "metadata": {},
   "source": [
    "# ML Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0017c",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mord\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befedf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../../cleaned_inspections.csv')\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab6dc5",
   "metadata": {},
   "source": [
    "### Modeling Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c78921",
   "metadata": {},
   "source": [
    "#### A few options on binning:\n",
    "\n",
    "1. Regression on the raw score\n",
    "    - What it does: Predicts the exact numeric score (0–100+).\n",
    "    - Why it’s powerful: Uses the full continuum of the target, so you’re not discarding any nuance.\n",
    "    - Trade‑off: You have to choose your pass/fail or A/B/C thresholds after you fit the model (but you can even tune those thresholds on a hold‑out set).\n",
    "\n",
    "2. Ordinal‑aware multi‑class\n",
    "    - What it does: Predicts ordered buckets (e.g. A/B/C → 0/1/2) while explicitly modeling their order.\n",
    "    - Why it helps: You still collapse the score into 3 groups, but your loss function “knows” that mis‑predicting A→B is a smaller error than A→C. That extra structure often boosts classification performance versus treating classes as unrelated.\n",
    "\n",
    "3. Plain multi‑class\n",
    "    - What it does: Predicts A, B, or C as independent labels.\n",
    "    - Why it’s weaker: Loses both granularity (all within‑class differences) and ordering information.\n",
    "\n",
    "4. Binary (fail/pass)\n",
    "    - What it does: Predicts whether score ≥ 28.\n",
    "    - Why it’s simplest: Straightforward, but you throw out nearly all of the score’s information (e.g. you treat a 27 the same as a 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30f736",
   "metadata": {},
   "source": [
    "#### A starting pathway for choosing the target approach:\n",
    "\n",
    "1. **Start simple with the binary flag**\n",
    "\n",
    "   * Create\n",
    "\n",
    "     ```python\n",
    "     df['failing'] = (df.score >= 28).astype(int)\n",
    "     ```\n",
    "   * Train a classifier (e.g. logistic regression or random forest) and evaluate ROC‑AUC / F1 on “fail.”\n",
    "   * You’ll get a baseline that directly answers “who fails?” with minimal fuss.\n",
    "\n",
    "2. **If you need more insight, step up to ordinal**\n",
    "\n",
    "   * Map scores into A/B/C:\n",
    "\n",
    "     ```python\n",
    "     bins = [ -1, 13, 27, float('inf') ]\n",
    "     labels = ['A','B','C']\n",
    "     df['grade'] = pd.cut(df.score, bins=bins, labels=labels)\n",
    "     ```\n",
    "   * Use an **ordinal** model (e.g. `statsmodels`’ OrdinalLogit) or transform into multiple binary tasks (cumulative link).\n",
    "   * This lets you exploit the fact that mis‐classifying A→B is “less wrong” than A→C.\n",
    "\n",
    "3. **Regression if you really care about exact scores**\n",
    "\n",
    "   * Predict `score` directly, then choose your cutoff(s) in post‑processing.\n",
    "   * You can even treat the cutoff as a hyperparameter and tune it on your validation set for best classification metrics.\n",
    "\n",
    "\n",
    "##### Why not jump straight to multi‑class/ordinal?\n",
    "\n",
    "* **Complexity**: True ordinal methods require special loss functions or libraries.\n",
    "* **Data needs**: More classes mean fewer examples per class, which can hurt performance.\n",
    "* **Interpretability**: Stakeholders often just want “pass/fail.”\n",
    "\n",
    "\n",
    "##### How ordinal vs. numeric thresholding differ\n",
    "\n",
    "* A **plain multi‑class tree** treats A, B, C as unrelated labels.  You’d need to encode order (e.g. A→0, B→1, C→2) and accept that your model is really doing regression on those integers.\n",
    "* A **true ordinal** approach (cumulative link models, ordinal forest, etc.) explicitly penalizes “distance” between predicted and true classes in its loss.\n",
    "\n",
    "\n",
    "**Bottom line:**\n",
    "\n",
    "* **If your goal is simply “which restaurants fail?”**, go with the binary flag at 28.\n",
    "* **If you want richer predictions on letter grade**, do the ordinal multi‑class next (but be aware you’ll need an ordinal‐aware method to fully exploit ordering).\n",
    "* **If you care about exact score predictions (and may want different thresholds later)**, build a regression model and threshold afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d97cd",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115132c6",
   "metadata": {},
   "source": [
    "#### Classes To Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3609463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Helper:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.target = 'score'\n",
    "\n",
    "    def pass_fail_bins(self) -> pd.DataFrame:\n",
    "        if self.target == 'score':    \n",
    "            self.df['failing'] = (self.df[self.target] >= 28).astype(int)\n",
    "            self.df.drop(columns = self.target, inplace = True)\n",
    "            self.target = 'failing'\n",
    "            return self\n",
    "        else:\n",
    "            print('Could not finish. Please ensure .ordinal_bins() has not already be run.')\n",
    "\n",
    "    def ordinal_bins(self) -> pd.DataFrame:\n",
    "        if self.target == 'score':\n",
    "            bins = [-1, 13, 27, float('inf')]\n",
    "            labels = [0, 1, 2]  # A=0, B=1, C=2\n",
    "            self.df['grade'] = pd.cut(self.df[self.target], bins = bins, labels = labels).astype(int)\n",
    "            self.df.drop(columns = [self.target], inplace = True)\n",
    "            self.target = 'grade'\n",
    "            return self\n",
    "        else:\n",
    "            print('Could not finish. Please ensure .pass_fail_bins() has not already be run.')\n",
    "\n",
    "    def target_split(self) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        return self.df.drop(columns = [self.target]), self.df[self.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Training:\n",
    "    '''\n",
    "        Logs experiments to a per-user CSV. Each row contains:\n",
    "        - timestamp (ISO)\n",
    "        - run_id (UUID4)\n",
    "        - model_name (class name)\n",
    "        - params (JSON dict)\n",
    "        - metrics (JSON dict)\n",
    "        - extra (JSON dict for anything else you want to track)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, classmate_name: str, log_dir = Path('logs')):\n",
    "        self.classmate = classmate_name\n",
    "        log_dir.mkdir(exist_ok = True)\n",
    "        log_stem = f'{classmate_name}_models.csv'\n",
    "        self.log_path = log_dir / log_stem\n",
    "\n",
    "        # If the file doesn’t exist, write a header\n",
    "        if not self.log_path.is_file():\n",
    "            with open(self.log_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['timestamp','run_id','model_name','params','metrics','extra'])\n",
    "\n",
    "    def log(self,\n",
    "            model=None,\n",
    "            *,\n",
    "            model_name: str = None,\n",
    "            params: dict = None,\n",
    "            metrics: dict = None,\n",
    "            extra: dict = None\n",
    "        ):\n",
    "        '''\n",
    "        Write one experiment record.\n",
    "\n",
    "        If you pass a scikit‑learn–style `model`, its .get_params() will be recorded automatically.\n",
    "        Otherwise, supply model_name and params explicitly.\n",
    "        '''\n",
    "        # Determine the name\n",
    "        name = model_name or (model.__class__.__name__ if model is not None else '<unknown>')\n",
    "\n",
    "        # Get params\n",
    "        if params is None:\n",
    "            if model is not None and hasattr(model, 'get_params'):\n",
    "                params = getattr(model, 'get_params')\n",
    "            else:\n",
    "                try:\n",
    "                    params = getattr(model, '__dict__', {})\n",
    "                except:\n",
    "                    params = {}\n",
    "        # Default metrics/extra\n",
    "        metrics = metrics or {}\n",
    "        extra   = extra   or {}\n",
    "        row = [\n",
    "            dt.datetime.now(dt.UTC).isoformat(),\n",
    "            str(uuid.uuid4()),\n",
    "            name,\n",
    "            json.dumps(params,  separators=('',','':'), default=str),\n",
    "            json.dumps(metrics, separators=(',',':'), default=str),\n",
    "            json.dumps(extra,   separators=(',',':'), default=str),\n",
    "        ]\n",
    "\n",
    "        with open(self.log_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_and_log_mord(self, X, y, test_size, **mord_kwargs):\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size = test_size, stratify = y)\n",
    "        m = mord.LogisticIT(**mord_kwargs).fit(Xtr, ytr)\n",
    "        preds = m.predict(Xte)\n",
    "        metrics = {'accuracy': accuracy_score(yte, preds)}\n",
    "        self.log(model=m, metrics=metrics)\n",
    "        return print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b58241",
   "metadata": {},
   "source": [
    "#### Using ML_Helper\n",
    "\n",
    "- Pass in a df to the constructor and save it to a variable of your choice: `ml = ML_Helper(df)`  \n",
    "\n",
    "- Call either `ml.pass_fail_bins()` or `ml.ordinal_bins()`, or neither for full score regression. You cannot call both, it won't work.  \n",
    "    + This step is to take our target variable (which is the inspection score) and either convert it to a pass fail, or to the letter grade  \n",
    "\n",
    "- Lastly, call `X, y = ml.target_split()` to get an automatic split of the X and y variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8261c",
   "metadata": {},
   "source": [
    "### Run Trainings Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7282c",
   "metadata": {},
   "source": [
    "After every run, please make sure to create a Log_Training object and run train_and_log_mord() to save the results of your model training/experimentation to a CSV log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763ac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = ML_Helper(df)\n",
    "# ml.pass_fail_bins()\n",
    "# ml.ordinal_bins()\n",
    "X, y = ml.target_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f88dd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "264770    1\n",
       "264771    0\n",
       "264772    0\n",
       "264773    0\n",
       "264774    0\n",
       "Name: failing, Length: 264775, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b40c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
